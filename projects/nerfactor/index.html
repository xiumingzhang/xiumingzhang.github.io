<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html>
  <head>
    <title>NeRFactor</title>
    <link rel="stylesheet" href="./style.css" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="icon" type="image/png" href="assets/logos/csail.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Facebook -->
    <meta property="og:image" content="assets/images/cover.jpg" />
    <meta property="og:image:type" content="image/jpeg" />
    <meta property="og:image:width" content="1024" />
    <meta property="og:image:height" content="512" />
    <meta property="og:type" content="website" />
    <meta
      property="og:url"
      content="http://people.csail.mit.edu/xiuming/projects/nerfactor/"
    />
    <meta property="og:title" content="NeRFactor" />
    <meta property="og:description" content="Project page for NeRFactor." />
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="NeRFactor" />
    <meta name="twitter:description" content="Project page for NeRFactor." />
    <meta name="twitter:image" content="assets/images/cover.jpg" />
  </head>

  <body>
    <h1>
      <b>NeRFactor</b>: Neural Factorization of Shape and Reflectance<br />Under
      an Unknown Illumination
    </h1>
    <div class="venue_card">
      <p class="venue">TOG 2021 (Proc. SIGGRAPH Asia)</p>
    </div>

    <div class="author_card">
      <p class="author">
        <a target="_blank" href="http://people.csail.mit.edu/xiuming/"
          >Xiuming Zhang</a
        >&nbsp;<sup>1</sup>&emsp;&emsp;
        <a target="_blank" href="https://pratulsrinivasan.github.io/"
          >Pratul P. Srinivasan</a
        >&nbsp;<sup>2</sup>&emsp;&emsp;
        <a target="_blank" href="https://boyangdeng.com/">Boyang Deng</a
        >&nbsp;<sup>2</sup>&emsp;&emsp;
        <a target="_blank" href="http://www.pauldebevec.com/">Paul Debevec</a
        >&nbsp;<sup>2</sup>&emsp;&emsp;
        <a target="_blank" href="http://billf.mit.edu/">William T. Freeman</a
        >&nbsp;<sup>1,&nbsp;2</sup>&emsp;&emsp;
        <a target="_blank" href="https://jonbarron.info/">Jonathan T. Barron</a
        >&nbsp;<sup>2</sup>
      </p>
      <table class="authoraffliation">
        <tr>
          <td>
            <sup>1</sup>&nbsp;<a
              target="_blank"
              href="https://www.csail.mit.edu/"
              >MIT CSAIL</a
            >
          </td>
          <td>
            <sup>2</sup>&nbsp;<a target="_blank" href="https://research.google/"
              >Google Research</a
            >
          </td>
        </tr>
        <tr>
          <td>
            <img src="assets/logos/mit.png" height="40em" />&emsp;<img
              src="assets/logos/csail.png"
              height="40em"
            />
          </td>
          <td><img src="assets/logos/google_research.png" height="40em" /></td>
        </tr>
      </table>
    </div>

    <div class="card">
      <h2>Abstract</h2>
      <table>
        <tr class="padbottom">
          We address the problem of recovering the shape and spatially-varying
          reflectance of an object from multi-view images (and their camera
          poses) of an object illuminated by one unknown lighting condition.
          This enables the rendering of novel views of the object under
          arbitrary environment lighting and editing of the object's material
          properties. The key to our approach, which we call Neural Radiance
          Factorization (NeRFactor), is to distill the volumetric geometry of a
          Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation
          of the object into a surface representation and then jointly refine
          the geometry while solving for the spatially-varying reflectance and
          environment lighting. Specifically, NeRFactor recovers 3D neural
          fields of surface normals, light visibility, albedo, and Bidirectional
          Reflectance Distribution Functions (BRDFs) without any supervision,
          using only a re-rendering loss, simple smoothness priors, and a
          data-driven BRDF prior learned from real-world BRDF measurements. By
          explicitly modeling light visibility, NeRFactor is able to separate
          shadows from albedo and synthesize realistic soft or hard shadows
          under arbitrary lighting conditions. NeRFactor is able to recover
          convincing 3D models for free-viewpoint relighting in this challenging
          and underconstrained capture setup for both synthetic and real scenes.
          Qualitative and quantitative experiments show that NeRFactor
          outperforms classic and deep learning-based state of the art across
          various tasks. Our videos, code, and data are available at
          people.csail.mit.edu/xiuming/projects/nerfactor/.
        </tr>
        <tr>
          <img src="assets/images/teaser.jpg" width="100%" />
        </tr>
        <tr>
          <p class="caption">
            Given a set of posed images of an object captured from multiple
            views under just one unknown illumination condition (left), Neural
            Radiance Factorization (NeRFactor) is able to factorize the scene
            into 3D neural fields of surface normals, light visibility, albedo,
            and material (center), which enables applications such as
            free-viewpoint relighting and material editing (right).
          </p>
        </tr>
      </table>
    </div>

    <div class="card">
      <h2>Paper</h2>
      <p class="pub">
        <b
          >NeRFactor: Neural Factorization of Shape and Reflectance Under an
          Unknown Illumination</b
        ><br />
        <a target="_blank" href="http://people.csail.mit.edu/xiuming/"
          >Xiuming Zhang</a
        >,
        <a target="_blank" href="https://pratulsrinivasan.github.io/"
          >Pratul P. Srinivasan</a
        >, <a target="_blank" href="https://boyangdeng.com/">Boyang Deng</a>,
        <a target="_blank" href="http://www.pauldebevec.com/">Paul Debevec</a>,
        <a target="_blank" href="http://billf.mit.edu/">William T. Freeman</a>,
        <a target="_blank" href="https://jonbarron.info/">Jonathan T. Barron</a>
        <br />
        <b>TOG 2021</b> (Proc. SIGGRAPH Asia)
        <br />
        <a target="_blank" href="https://arxiv.org/pdf/2106.01970.pdf">arXiv</a
        >&emsp;/&emsp;
        <a
          target="_blank"
          href="https://dl.acm.org/doi/abs/10.1145/3478513.3480496"
          >Publisher</a
        >&emsp;/&emsp;
        <a
          target="_blank"
          href="http://people.csail.mit.edu/xiuming/docs/bib/nerfactor.bib"
          >BibTeX</a
        >
      </p>
      <!--
      <p>
      BibTeX:
      <pre>
@article{zhang2021nerfactor,
  title={NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination},
  author={Zhang, Xiuming and Srinivasan, Pratul P and Deng, Boyang and Debevec, Paul and Freeman, William T and Barron, Jonathan T},
  journal={arXiv preprint arXiv:2106.01970},
  year={2021}
}
      </pre>
      </p>
      -->
      <!--
        <p class="caption"><i>v0 (06/01/2021)</i></p>
      -->
      <p class="caption"><i>v1 (12/22/2021)</i></p>
    </div>

    <div class="card">
      <h2>Video</h2>
      <p>
        Downloads:&emsp;
        <a
          target="_blank"
          href="https://drive.google.com/file/d/1nPLhzUh7byjDlKOBKvaaeCGGGKlND3OL/view?usp=sharing"
          >1080p (212 MB)</a
        >&ensp; / &ensp;
        <a
          target="_blank"
          href="https://drive.google.com/file/d/1FmhKl-BaJQHAlQiw8HO_KGo_OMuaCOqf/view?usp=sharing"
          >720p (154 MB)</a
        >
      </p>
      <div style="position: relative; padding-top: 60%">
        <iframe
          src="https://www.youtube.com/embed/UUVSPJlwhPg"
          allowfullscreen
          style="position: absolute; top: 0; left: 0"
        ></iframe>
      </div>
      <p class="caption"><i>v0 (06/01/2021)</i></p>
    </div>

    <div class="card">
      <h2>Talk</h2>
      <div style="position: relative; padding-top: 60%">
        <iframe
          src="https://www.youtube.com/embed/vt_zQTxqmlk"
          allowfullscreen
          style="position: absolute; top: 0; left: 0"
        ></iframe>
      </div>
      <p class="caption"><i>v1 (12/22/2021)</i></p>
    </div>

    <div class="card">
      <h2>Code</h2>
      <p>
        <a target="_blank" href="https://github.com/google/nerfactor"
          >This GitHub repository</a
        >
        includes code for:
      </p>
      <ul>
        <li>
          Data rendering and processing (so that you can render your own scenes
          or process your own captures);
        </li>

        <li>
          Model training and validation (so that you can modify the model and
          train it on your own data); and
        </li>
        <li>
          Model inference/testing (so that you can quickly try out our trained
          models).
        </li>
      </ul>
    </div>

    <div class="card">
      <h2>Downloads</h2>
      <p>
        All subsequent folders and files are specified w.r.t.
        <a
          target="_blank"
          href="https://drive.google.com/drive/folders/1-9HhqWwJhTBjUZqttLafKo72fNl4sB5E"
          >this Google Drive root</a
        >.
      </p>

      <p>
        If you want to try our trained models, see "Pre-Trained Models." If you
        want to use our rendered/processed data, see "Data." If you want to
        render your own data, also see "Metadata."
      </p>

      <h3>Pre-Trained Models <mark>(new as of 07/17/2022)</mark></h3>
      <p>
        There are four types of pre-trained models in NeRFactor. The first is
        the MERL BRDF model, which we release here. The second are the NeRF
        models that generate the surfaces we start with. We are not releasing
        these but you can train these NeRF models and then generate the surfaces
        using our code. The third are the NeRFactor shape pre-training models
        that learn to just reproduce the NeRF surfaces. We are not releasing
        these but you can train them easily with our code. The fourth are the
        final NeRFactor models (<code>hotdog</code>, <code>ficus</code>,
        <code>lego</code>, and <code>drums</code>), which we release here. See
        <code>./pretrained_models.zip</code>.
      </p>

      <h3>Data</h3>
      <p>
        We release the images rendered from the synthetic scenes above and the
        real images (<code>vasedeck</code> and <code>pinecone</code> from NeRF)
        processed to be compatible with our model's data format:
      </p>
      <table>
        <tbody>
          <tr>
            <td>Rendered Images</td>
            <td>Real Images</td>
          </tr>
          <tr>
            <td><code>./rendered-images/</code></td>
            <td><code>./real-images/</code></td>
          </tr>
        </tbody>
      </table>

      <h3>Metadata</h3>
      <p>
        We release the four .blend scenes (modified from what NeRF released):
        <code>lego</code>, <code>hotdog</code>, <code>ficus</code>, and
        <code>drums</code>, the training/testing light probes used in the paper,
        and the training/validation/testing cameras (exactly the same as what
        NeRF released):
      </p>
      <table>
        <tbody>
          <tr>
            <td>Scenes</td>
            <td>Light Probes</td>
            <td>Cameras</td>
          </tr>
          <tr>
            <td><code>./blender-scenes.zip</code></td>
            <td><code>./light-probes.zip</code></td>
            <td><code>./cameras.zip</code></td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="card">
      <h2>All Video Results</h2>
      <p>
        We release the video results of NeRFactor, its material editing, the
        NeRFactor variants used in our ablation studies, and Oxholm & Nishino
        [2014].
      </p>

      <h3>NeRFactor</h3>
      <p>
        Here are our results on all four synthetic scenes and two real captures:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
          </tr>
          <tr class="padbottom">
            <td>
              <iframe
                src="https://www.youtube.com/embed/RMUTfDnlMuE"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/YOdhyHl0uec"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/mMqS9Ek1NUI"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
          <tr>
            <td><code>ficus_2188</code></td>
            <td><code>vasedeck</code></td>
            <td><code>pinecone</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/8xnwowUQy7M"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/dIjKj_GOcoU"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/615faNRV-_Y"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>

      <h3>Material Editing via NeRFactor</h3>
      <p>
        Here are our material editing results on all four synthetic scenes and
        two real captures:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
          </tr>
          <tr class="padbottom">
            <td>
              <iframe
                src="https://www.youtube.com/embed/F4wVn8rR9Ac"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/tMaUAz_tznY"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/yZ8yUHsZmsY"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
          <tr>
            <td><code>ficus_2188</code></td>
            <td><code>vasedeck</code></td>
            <td><code>pinecone</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/JemF9E1fkd4"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/thhQmVb7N98"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/EAkalvpmU5s"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>

      <h3>Ablation Studies</h3>
      <p>
        Here are the NeRFactor variants' results on all four synthetic scenes.
      </p>
      <h4>Directly Using NeRF's Shape</h4>
      <p>
        This model variant fixes the shape to NeRF's, optimizing only the
        reflectance and illumination:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
            <td><code>ficus_2188</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/ZvrCRHXS1Fw"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/6uqydXRGnSI"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/8QJFyiQgsvA"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/gwW1F_TJ1W0"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>
      <h4>No Smoothness Regularization</h4>
      <p>This model variant uses no smoothness regularization:</p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
            <td><code>ficus_2188</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/yzaAjazEABQ"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/QIRT_J3Vq88"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/2ID4Sph1-PY"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/7dZ8vKB1Xxc"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>
      <h4>Using Microfacet BRDFs</h4>
      <p>
        This model variant uses microfacet BRDFs instead of the learned BRDFs:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
            <td><code>ficus_2188</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/W-DXgCXYGt4"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/3ftZ77IRFnA"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/QbTWVV8b38M"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/AoK07Yf2jFY"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>
      <h4>No Geometry Pretraining</h4>
      <p>
        This model variant trains the surface normal and light visibility MLPs
        from scratch, together with the reflectance and lighting:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
            <td><code>ficus_2188</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/TG2nWMoPBug"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/AURZ6cyhgBI"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/0qYOTtTnrpA"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/Orv6Wyf1eL8"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>

      <h3>Oxholm & Nishino [2014]</h3>
      <p>
        Here is how NeRFactor compares with an enhanced version of Oxholm &
        Nishino [2014] (implemented by ourselves due to the lack of the source
        code) on all four synthetic scenes:
      </p>
      <table>
        <tbody>
          <tr>
            <td><code>lego_3072</code></td>
            <td><code>hotdog_2163</code></td>
            <td><code>drums_3072</code></td>
            <td><code>ficus_2188</code></td>
          </tr>
          <tr>
            <td>
              <iframe
                src="https://www.youtube.com/embed/HvHWxRzniY0"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/mOblj68xHic"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/z1m5cAYi0zM"
                allowfullscreen
              ></iframe>
            </td>
            <td>
              <iframe
                src="https://www.youtube.com/embed/EaVaHPqnzCo"
                allowfullscreen
              ></iframe>
            </td>
          </tr>
        </tbody>
      </table>
      <p>
        †Both this enhanced version and the original model require ground-truth
        lighting, which we provided.
      </p>
    </div>

    <div class="copyright_card">
      <p>Copyright &copy; 2021 Paper Authors. All Rights Reserved.</p>
    </div>
  </body>
</html>
